{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "DLNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2kZTqtHPXRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e706757d-ff96-477e-bbe9-b240ab577437"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y0MuZpvPSoG"
      },
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "from librosa import display\n",
        "import matplotlib.pyplot as plt\n",
        "from DLNet_functions import PreprocessWrapper\n",
        "from models import ModelType, ModelBuilder\n",
        "import tensorflow as tf\n",
        "from time import strftime\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "# autotune computation\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "DATA_PATH = '/content/drive/MyDrive'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U5gsogwPSoL",
        "outputId": "b12f419f-470f-493a-d980-bee8b38595dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " # Read config from dataset:\n",
        "json_file = os.path.join(DATA_PATH, 'tf_dataset_uncompr_wav', 'DLNet_config.json')\n",
        "with open(json_file, \"r\") as read_file:\n",
        "            config = json.load(read_file)\n",
        "# Path to dataset_config:\n",
        "ds_config: str = os.path.join(DATA_PATH, 'dataset_config.json')\n",
        "# Creater wrapper object:\n",
        "wrapper: PreprocessWrapper = PreprocessWrapper(config, ds_config)\n",
        "# Print Config:\n",
        "config"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'audio_length': 1,\n",
              " 'binary': True,\n",
              " 'calculate_mel': False,\n",
              " 'center': True,\n",
              " 'classes': ['compressed_wav', 'uncompr_wav'],\n",
              " 'filter_signal': False,\n",
              " 'hop_length': 256,\n",
              " 'input_shape': [513, 173, 1],\n",
              " 'mono': True,\n",
              " 'n_fft': 1024,\n",
              " 'n_frames': 173,\n",
              " 'n_mels': 64,\n",
              " 'pad_mode': 'reflect',\n",
              " 'power': 2.0,\n",
              " 'random_seed': 10,\n",
              " 'sr': 44100,\n",
              " 'time_stamp': '14_02_2021_13_23',\n",
              " 'win_length': 512,\n",
              " 'window': 'hamm'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwA9IoGvXyT0"
      },
      "source": [
        "# Path to datasets:\n",
        "# AAC\n",
        "path_aac_train = os.path.join(DATA_PATH, 'tf_dataset_aac_128', 'aac_128_train_set')\n",
        "path_aac_test = os.path.join(DATA_PATH, 'tf_dataset_aac_128', 'aac_128_train_set')\n",
        "\n",
        "# MP3 32k\n",
        "path_mp3_32k_train = os.path.join(DATA_PATH, 'tf_dataset_mp3_32k', 'mp3_32k_train_set')\n",
        "path_mp3_32k_test = os.path.join(DATA_PATH, 'tf_dataset_mp3_32k', 'mp3_32k_train_set')\n",
        "\n",
        "# MP3 160k\n",
        "path_mp3_160k_train = os.path.join(DATA_PATH, 'tf_dataset_mp3_160k', 'mp3_160k_train_set')\n",
        "path_mp3_160k_test = os.path.join(DATA_PATH, 'tf_dataset_mp3_160k', 'mp3_160k_train_set')\n",
        "\n",
        "# MP3 192k\n",
        "path_mp3_192k_train = os.path.join(DATA_PATH, 'tf_dataset_mp3_192k', 'mp3_192k_train_set')\n",
        "path_mp3_192k_test = os.path.join(DATA_PATH, 'tf_dataset_mp3_192k', 'mp3_192k_train_set')\n",
        "\n",
        "# MP3 320k\n",
        "path_mp3_320k_train = os.path.join(DATA_PATH, 'tf_dataset_mp3_320k', 'mp3_320k_train_set')\n",
        "path_mp3_320k_test = os.path.join(DATA_PATH, 'tf_dataset_mp3_320k', 'mp3_320k_train_set')\n",
        "\n",
        "# AC3\n",
        "path_ogg_vbr_train = os.path.join(DATA_PATH, 'tf_dataset_ogg_vbr', 'ogg_vbr_train_set')\n",
        "path_ogg_vbr_test = os.path.join(DATA_PATH, 'tf_dataset_ogg_vbr', 'ogg_vbr_train_set')\n",
        "\n",
        "# Uncompressed/Lossless\n",
        "path_uncompr_train = os.path.join(DATA_PATH, 'tf_dataset_uncompr_wav', 'uncompr_wav_train_set')\n",
        "path_uncompr_test = os.path.join(DATA_PATH, 'tf_dataset_uncompr_wav', 'uncompr_wav_train_set')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeMDGkRMXjHP"
      },
      "source": [
        "# Load dataset:\n",
        "# Compressed\n",
        "train_compr = wrapper.load_tf_dataset(path_aac_train)\n",
        "test_compr = wrapper.load_tf_dataset(path_aac_test)\n",
        "\n",
        "# Uncompressed/Lossless\n",
        "train_uncompr = wrapper.load_tf_dataset(path_uncompr_train)\n",
        "test_uncompr = wrapper.load_tf_dataset(path_uncompr_test)\n",
        "\n",
        "# Concatenate datasets:\n",
        "train_dataset = train_uncompr.concatenate(train_compr)\n",
        "test_dataset = test_uncompr.concatenate(test_compr)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR9MpUUkc1Wr",
        "outputId": "4a882eda-159f-48f5-9a2d-502aea96b05c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ConcatenateDataset shapes: ((513, 173, 1), (2,)), types: (tf.float32, tf.uint8)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocaBr3OiPSoO"
      },
      "source": [
        "train_size = len(train_dataset)\n",
        "test_size = len(test_dataset)\n",
        "eval_size = int(.1*train_size)\n",
        "batch_size = 64\n",
        "\n",
        "# Shuffel train data:\n",
        "train_dataset = train_dataset.shuffle(buffer_size=train_size)\n",
        "\n",
        "# Split train into train and eval set:\n",
        "eval_dataset = train_dataset.take(eval_size)\n",
        "eval_dataset = eval_dataset.batch(batch_size).prefetch(AUTOTUNE)\n",
        "\n",
        "# Train dataset\n",
        "train_dataset = train_dataset.skip(eval_size)\n",
        "train_dataset = train_dataset.shuffle(train_size - eval_size)\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "train_dataset = train_dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "# Prepare test dataset\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(AUTOTUNE)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1mfLwU_c-1s",
        "outputId": "aec3c209-a324-4272-a1d2-ff286e1e8575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, 513, 173, 1), (None, 2)), types: (tf.float32, tf.uint8)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zum-gT5ZRIX3"
      },
      "source": [
        "MODEL_TYPE = ModelType.HENNEQUIN\n",
        "model_builder = ModelBuilder(MODEL_TYPE, config['input_shape'],\n",
        "                             config['classes'])\n",
        "model = model_builder.get_model()\n",
        "\n",
        "# Define metrics\n",
        "metrics = [tf.keras.metrics.TrueNegatives(),\n",
        "           tf.keras.metrics.TruePositives(),\n",
        "           tf.keras.metrics.FalseNegatives(),\n",
        "           tf.keras.metrics.FalsePositives(),\n",
        "           tf.keras.metrics.Precision(),\n",
        "           tf.keras.metrics.Recall(),\n",
        "           tf.keras.metrics.CategoricalAccuracy()\n",
        "           ]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC-5lFN0PSoP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "4f81eced-f4ba-4131-964d-db63949b69e9"
      },
      "source": [
        "# compile model\n",
        "n_epochs = 2\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit(train_dataset, epochs=n_epochs,\n",
        "                    validation_data=eval_dataset)\n",
        "\n",
        "model.evaluate(test_dataset, batch_size=64)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8b9612558410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m history = model.fit(train_dataset, epochs=n_epochs,\n\u001b[0;32m----> 9\u001b[0;31m                     validation_data=eval_dataset)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: 2 root error(s) found.\n  (0) Not found:  /content/drive/MyDrive/tf_dataset_uncompr_wav/uncompr_wav_train_set/2476661412491365324/00000000.shard/00000000.snapshot; No such file or directory\n\t [[node IteratorGetNext (defined at <ipython-input-22-8b9612558410>:9) ]]\n  (1) Not found:  /content/drive/MyDrive/tf_dataset_uncompr_wav/uncompr_wav_train_set/2476661412491365324/00000000.shard/00000000.snapshot; No such file or directory\n\t [[node IteratorGetNext (defined at <ipython-input-22-8b9612558410>:9) ]]\n\t [[IteratorGetNext/_2]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1588]\n\nFunction call stack:\ntrain_function -> train_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDJAlyLJPSoQ"
      },
      "source": [
        "# setup plot\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(16,4))\n",
        "\n",
        "# plot loss\n",
        "ax[0].plot(range(n_epochs), history.history['loss'])\n",
        "ax[0].plot(range(n_epochs), history.history['val_loss'])\n",
        "ax[0].set_ylabel('loss'), ax[0].set_title('train_loss vs val_loss')\n",
        "\n",
        "# plot accuracy\n",
        "ax[1].plot(range(n_epochs), history.history['categorical_accuracy'])\n",
        "ax[1].plot(range(n_epochs), history.history['val_categorical_accuracy'])\n",
        "ax[1].set_ylabel('accuracy'), ax[1].set_title('train_acc vs val_acc')\n",
        "\n",
        "# plot adjustement\n",
        "for a in ax:\n",
        "    a.grid(True)\n",
        "    a.legend(['train','val'], loc=4)\n",
        "    a.set_xlabel('num of Epochs')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}