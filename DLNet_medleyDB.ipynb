{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "DLNet.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gd2EjmDhoEl"
      },
      "source": [
        "### Mount drive to datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2kZTqtHPXRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a1af0e-17cd-4e86-a6fe-2d7e3431ddec"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KDqTnB4hu15"
      },
      "source": [
        "### Import libraries and set `DATA_PATH`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y0MuZpvPSoG"
      },
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "from librosa import display\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from time import strftime\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "# autotune computation\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "DATA_PATH = '/content/drive/MyDrive'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yn_Zm1eh7Rd"
      },
      "source": [
        "### All paths to possible codecs:\n",
        "All codec datasets are created for a binary problem. <br>\n",
        "See configs in DLNet_config.json for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwA9IoGvXyT0"
      },
      "source": [
        "# Path to datasets:\n",
        "# Full dataset (with all codecs)\n",
        "path_medley_train = os.path.join(DATA_PATH, 'tf_dataset_MedleyDB', 'MedleyDB_train_set')\n",
        "path_medley_test = os.path.join(DATA_PATH, 'tf_dataset_MedleyDB', 'MedleyDB_test_set')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im76779riuFO"
      },
      "source": [
        "### Read `config` and create `wrapper` object:\n",
        "Each dataset has a config file included with the dataset configurations.<br>\n",
        "Dataset configurations have to match to concatenate the datasets later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U5gsogwPSoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39b697f5-b5d2-4032-f667-ea3dcd2da9fe"
      },
      "source": [
        " # Read configs from dataset:\n",
        "json_file_codec =  os.path.join(os.path.split(path_medley_train)[0], 'DLNet_config.json')\n",
        "with open(json_file_codec, \"r\") as read_file:\n",
        "            config = json.load(read_file)\n",
        "# Print Config:\n",
        "print(json.dumps(config, sort_keys=False, indent=4))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"audio_length\": 1,\n",
            "    \"binary\": false,\n",
            "    \"calculate_mel\": false,\n",
            "    \"center\": true,\n",
            "    \"classes\": [\n",
            "        \"mp3_32k\",\n",
            "        \"mp3_160k\",\n",
            "        \"mp3_192k\",\n",
            "        \"mp3_320k\",\n",
            "        \"aac_128\",\n",
            "        \"ogg_vbr\",\n",
            "        \"uncompr_wav\"\n",
            "    ],\n",
            "    \"filter_signal\": false,\n",
            "    \"hop_length\": 256,\n",
            "    \"input_shape\": [\n",
            "        513,\n",
            "        173,\n",
            "        1\n",
            "    ],\n",
            "    \"mono\": true,\n",
            "    \"n_fft\": 1024,\n",
            "    \"n_frames\": 173,\n",
            "    \"n_mels\": 64,\n",
            "    \"pad_mode\": \"reflect\",\n",
            "    \"power\": 2.0,\n",
            "    \"random_seed\": 10,\n",
            "    \"sr\": 44100,\n",
            "    \"time_stamp\": \"14_02_2021_16_41\",\n",
            "    \"win_length\": 512,\n",
            "    \"window\": \"hamm\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM8gxj4ajdqQ"
      },
      "source": [
        "### Load chosen datasets and concatenate them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeMDGkRMXjHP"
      },
      "source": [
        "# Load dataset:\n",
        "# Codec1: (Compressed)\n",
        "train_dataset = tf.data.experimental.load(path_medley_train,\n",
        "                        (tf.TensorSpec(config['input_shape'],\n",
        "                                       dtype=tf.float32, name=None),\n",
        "                         tf.TensorSpec(len(config['classes']),\n",
        "                                       dtype=tf.uint8, name=None)),\n",
        "                        compression='GZIP')\n",
        "test_dataset = tf.data.experimental.load(path_medley_test,\n",
        "                        (tf.TensorSpec(config['input_shape'],\n",
        "                                       dtype=tf.float32, name=None),\n",
        "                         tf.TensorSpec(len(config['classes']),\n",
        "                                       dtype=tf.uint8, name=None)),\n",
        "                        compression='GZIP')\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml1i2p3tjnOP"
      },
      "source": [
        "### Print dataset info:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR9MpUUkc1Wr",
        "outputId": "05e8c814-bafe-4dba-f272-180704e92cdb"
      },
      "source": [
        "print(train_dataset)\n",
        "print(len(train_dataset))\n",
        "print(test_dataset)\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_LoadDataset shapes: ((513, 173, 1), (7,)), types: (tf.float32, tf.uint8)>\n",
            "23200\n",
            "<_LoadDataset shapes: ((513, 173, 1), (7,)), types: (tf.float32, tf.uint8)>\n",
            "6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6L60LYGj19d"
      },
      "source": [
        "### Prepare datasets:\n",
        "\n",
        "\n",
        "*   Shuffle `train_dataset`\n",
        "*   Take 10 % of `train_dataset` for evaluation: `eval_dataset`\n",
        "*   Shuffle and batch rest of `train_dataset`\n",
        "*   Batch `test_dataset`\n",
        "*   Batch size: 64\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocaBr3OiPSoO"
      },
      "source": [
        "train_size = len(train_dataset)\n",
        "test_size = len(test_dataset)\n",
        "eval_size = int(.1*train_size)\n",
        "batch_size = 64\n",
        "scaling_factor = .5\n",
        "\n",
        "# Shuffel train data:\n",
        "train_dataset = train_dataset.shuffle(buffer_size=int(train_size/3))\n",
        "\n",
        "# Split train into train and eval set:\n",
        "eval_dataset = train_dataset.take(int(scaling_factor*eval_size))\n",
        "eval_dataset = eval_dataset.batch(batch_size).prefetch(AUTOTUNE)\n",
        "\n",
        "# Train dataset\n",
        "train_dataset = train_dataset.skip(eval_size)\n",
        "train_dataset = train_dataset.take(int(scaling_factor*train_size))\n",
        "train_dataset = train_dataset.shuffle(train_size - eval_size)\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "train_dataset = train_dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "# Prepare test dataset\n",
        "test_dataset = test_dataset.take(int(scaling_factor*test_size)).batch(batch_size).prefetch(AUTOTUNE)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekQWRjrOkqjJ"
      },
      "source": [
        "### Create and compile CNN Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zum-gT5ZRIX3"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=config['input_shape']))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.GlobalMaxPool2D())\n",
        "model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.GaussianDropout(0.25))\n",
        "model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.GaussianDropout(0.25))\n",
        "model.add(tf.keras.layers.Dense(len(config['classes']), activation=\"softmax\"))\n",
        "\n",
        "# Define metrics\n",
        "metrics = [tf.keras.metrics.TrueNegatives(),\n",
        "           tf.keras.metrics.TruePositives(),\n",
        "           tf.keras.metrics.FalseNegatives(),\n",
        "           tf.keras.metrics.FalsePositives(),\n",
        "           tf.keras.metrics.Precision(),\n",
        "           tf.keras.metrics.Recall(),\n",
        "           tf.keras.metrics.CategoricalAccuracy()\n",
        "           ]\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=metrics)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avCQK_89ksTr"
      },
      "source": [
        "### Train and evaluate model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC-5lFN0PSoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "222d778c-96bc-4eeb-8f98-ae49a514e372"
      },
      "source": [
        "# fit model\n",
        "n_epochs = 17\n",
        "history = model.fit(train_dataset, epochs=n_epochs,\n",
        "                    validation_data=eval_dataset, verbose=1)\n",
        "\n",
        "model.evaluate(test_dataset, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDJAlyLJPSoQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "1c6a8f5f-3ac5-4326-d655-0749229c5174"
      },
      "source": [
        "# setup plot\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(16,4))\n",
        "\n",
        "# plot loss\n",
        "ax[0].plot(range(n_epochs), history.history['loss'])\n",
        "ax[0].plot(range(n_epochs), history.history['val_loss'])\n",
        "ax[0].set_ylabel('loss'), ax[0].set_title('train_loss vs val_loss')\n",
        "\n",
        "# plot accuracy\n",
        "ax[1].plot(range(n_epochs), history.history['accuracy'])\n",
        "ax[1].plot(range(n_epochs), history.history['val_accuracy'])\n",
        "ax[1].set_ylabel('accuracy'), ax[1].set_title('train_acc vs val_acc')\n",
        "\n",
        "# plot adjustement\n",
        "for a in ax:\n",
        "    a.grid(True)\n",
        "    a.legend(['train','val'], loc=4)\n",
        "    a.set_xlabel('num of Epochs')\n",
        "plt.show()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b9d0fbb31acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# setup plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plot loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m9mk8wqf4m4"
      },
      "source": [
        "### Model prediction as numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2y1aUZVf2EQ"
      },
      "source": [
        "y_test_prob = model.predict(test_dataset)\n",
        "y_test_pred = np.argmax(y_test_prob, axis=1)\n",
        "y_test_true = np.argmax(np.array([y for x, y in test_dataset.unbatch().as_numpy_iterator()]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMeUYne-fLin"
      },
      "source": [
        "### Print metrics with sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeN50E41e-QT"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print('Classification report:')\n",
        "print(classification_report(y_true=y_test_true, y_pred=y_test_pred,\n",
        "                            target_names=config['classes']))\n",
        "\n",
        "print('------------------------------------------------:')\n",
        "print('Evaluate model on test dataset:')\n",
        "hist_eval = model.evaluate(test_dataset)\n",
        "\n",
        "print('------------------------------------------------:')\n",
        "print('Evaluate model on eval dataset:')\n",
        "hist_eval = model.evaluate(eval_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0QHbCHVfRYM"
      },
      "source": [
        "### Confusion Matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeHmGGaWdKla"
      },
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label'), plt.xlabel('Predicted label')\n",
        "\n",
        "# call confusion matrix\n",
        "cm = tf.math.confusion_matrix(labels=y_test_true, predictions=y_test_pred)\n",
        "cm = cm.numpy()\n",
        "plot_confusion_matrix(cm, classes=config['classes'], normalize=False,\n",
        "                      title='Confusion matrix', cmap=plt.cm.Blues)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}